{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling Method Testing for Trajectory Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle\n",
    "import geopy.distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set display options\n",
    "pd.set_option('display.max_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grab a sample of trajectories to test\n",
    "trajectories = pd.read_csv('trajectories.csv',nrows=8000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check number of boats in sample\n",
    "len(list(trajectories.mmsi.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Interpolation Based on Full Boat Trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change time to datetime and round time to nearest 10 minutes\n",
    "trajectories['timestamp'] = pd.to_datetime(trajectories['timestamp'])\n",
    "trajectories['new_time'] = trajectories['timestamp'].dt.round('20min')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a data frame with uniform time stamps of 20 minutes\n",
    "date_range = []\n",
    "\n",
    "date1 = '2012-01-01 00:00:00'\n",
    "date2 = '2016-12-31 00:00:00'\n",
    "start = datetime.datetime.strptime(date1, '%Y-%m-%d %H:%M:%S')\n",
    "end = datetime.datetime.strptime(date2, '%Y-%m-%d %H:%M:%S')\n",
    "step = datetime.timedelta(minutes=20)\n",
    "while start <= end:\n",
    "    date_range.append(start)\n",
    "    start += step   \n",
    "    \n",
    "date_test = pd.DataFrame(date_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize a list of all the mmsi\n",
    "boat_list = list(trajectories.mmsi.unique())\n",
    "\n",
    "df_list = [] #initialize list for storing processed dataframes\n",
    "\n",
    "#loop through mmsi list, each column in the output will be all the lat-lon pairs for every possible time step\n",
    "\n",
    "for mmsi in boat_list:\n",
    "    \n",
    "    trajectories_test = trajectories[trajectories['mmsi']==mmsi] #filter on single boat\n",
    "\n",
    "    uniform_traj = pd.merge(left=date_test,right=trajectories_test,how='left',left_on=0,right_on='new_time')\n",
    "    \n",
    "    uniform_traj = uniform_traj[[0,'lat','lon']] #filter to only needed columns\n",
    "    uniform_traj.drop_duplicates(subset=[0],inplace=True) #drop duplicate time values, keep first record at each time\n",
    "    uniform_traj.reset_index(inplace=True) #reset index so concatenation will align\n",
    "    uniform_traj = uniform_traj.interpolate(method='linear') #perform linear interpolation\n",
    "    uniform_traj['lat_lon'] = uniform_traj[['lat','lon']].values.tolist() #convert lat-lon to list object\n",
    "    uniform_traj = uniform_traj[['lat_lon']] #keep just the lat lon values\n",
    "    uniform_traj.rename({'lat_lon':mmsi},axis=1,inplace=True) #rename column based on boat id\n",
    "    df_list.append(uniform_traj) #append data frame to df list\n",
    "    \n",
    "uniform_boats = pd.concat(df_list,axis=1)\n",
    "uniform_boats = pd.merge(left=date_test,right=uniform_boats,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generate a list of interpolated trajectories\n",
    "uniform_boats_col_list = list(uniform_boats.columns[1:]) #just names of boat columns\n",
    "\n",
    "interpolated_list = [] #initialize empty list for storing \n",
    "\n",
    "for col in uniform_boats_col_list:\n",
    "    test_df = uniform_boats[col]\n",
    "    test_list = test_df.tolist()\n",
    "    \n",
    "    traj = []\n",
    "    for i in test_list:\n",
    "        if str(i[0]) != 'nan':\n",
    "            traj.append(i)\n",
    "    \n",
    "    interpolated_list.append(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nRead it back by calling:\\nwith open ('outfile', 'rb') as fp:\\n    itemlist = pickle.load(fp)\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the list object\n",
    "with open('full_trajectories', 'wb') as fp:\n",
    "    pickle.dump(interpolated_list, fp)\n",
    "    \n",
    "'''\n",
    "Read it back by calling:\n",
    "with open ('outfile', 'rb') as fp:\n",
    "    itemlist = pickle.load(fp)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Trajectories into Trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first step is linear interpolation at the MMSI level\n",
    "#### Second step is measuring the speed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions needed for trip segmentation calculations\n",
    "\n",
    "#define distance function\n",
    "def distance_calc(df):\n",
    "    dist = geopy.distance.vincenty(df['lat_lon'], df['next_latlon']).km\n",
    "    return dist\n",
    "\n",
    "#create stationary flag based on speed threshold\n",
    "def station_flag(df):\n",
    "    '''creates a binary flag for when a boat is stationary'''\n",
    "    if df['km_per_hr'] < 0.3:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "#create a trip id tag based on stationarity of boat\n",
    "def trip_tag(df):\n",
    "    '''tags trajectories of movement with a trip id'''\n",
    "    if df['stationary_flag'] == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return df['trip_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trip_segmentation_prep(df,boat):\n",
    "    '''function for preprocessing dataframe'''\n",
    "    trajectories_test = trajectories[df['mmsi']==boat] #filter on single boat\n",
    "    trajectories_test = trajectories_test[['mmsi','new_time','lat','lon']] #filter only needed columns\n",
    "    \n",
    "    uniform_traj = pd.merge(left=date_test,right=trajectories_test,how='left',left_on=0,right_on='new_time')\n",
    "\n",
    "    uniform_traj = uniform_traj[[0,'lat','lon']] #filter to only needed columns\n",
    "    uniform_traj.drop_duplicates(subset=[0],inplace=True) #drop duplicate time values, keep first record at each time\n",
    "    uniform_traj.reset_index(inplace=True) #reset index so concatenation will align\n",
    "    uniform_traj = uniform_traj.interpolate(method='linear') #perform linear interpolation\n",
    "    \n",
    "    #returns index value of the first non NaN, which is where we want the trajectory to start from\n",
    "    uniform_traj = uniform_traj[uniform_traj['lat'].index.get_loc(uniform_traj['lat'].first_valid_index()):]\n",
    "    \n",
    "    #some preprocessing for the lat-lon columns\n",
    "    #create a new column with a tuple \n",
    "    uniform_traj['lat_lon'] = list(zip(uniform_traj['lat'],uniform_traj['lon']))\n",
    "    #create column of next location\n",
    "    uniform_traj['next_latlon'] = uniform_traj['lat_lon'].shift(-1)\n",
    "    #filter last row which will be a NaN\n",
    "    uniform_traj = uniform_traj[:-1]\n",
    "    \n",
    "    return uniform_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfshome/cb4102/.conda/envs/deeplearn/lib/python3.5/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Vincenty is deprecated and is going to be removed in geopy 2.0. Use `geopy.distance.geodesic` (or the default `geopy.distance.distance`) instead, which is more accurate and always converges.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170052967564863 processed\n",
      "105579478788595 processed\n",
      "130155297254428 processed\n",
      "108705926933614 processed\n",
      "209123096662320 processed\n",
      "251545767275046 processed\n",
      "221946427834059 processed\n",
      "224786612746287 processed\n",
      "146855328418227 processed\n",
      "138705062821220 processed\n",
      "472004530520 processed\n",
      "194671993660064 processed\n",
      "260351244058145 processed\n",
      "23770783250938 processed\n",
      "26740506301377 processed\n",
      "183690246677486 processed\n",
      "192665586399581 processed\n",
      "13275176023916 processed\n",
      "256792384000220 processed\n",
      "116578315107420 processed\n",
      "253844832070541 processed\n",
      "102029098096261 processed\n",
      "184417002225525 processed\n",
      "130679899491312 processed\n",
      "214743087662983 processed\n",
      "115904004134405 processed\n",
      "236614363512057 processed\n",
      "49946321646941 processed\n",
      "187863545115697 processed\n",
      "103998194181708 processed\n",
      "201416986422368 processed\n",
      "109983223516401 processed\n",
      "256926157663553 processed\n",
      "232049438520401 processed\n",
      "190845284027868 processed\n",
      "244602438606339 processed\n",
      "259125516190601 processed\n",
      "184994959626596 processed\n",
      "244511605554921 processed\n",
      "212820497932329 processed\n",
      "196896378492676 processed\n",
      "103576446797335 processed\n",
      "150151863317535 processed\n",
      "101554492253652 processed\n",
      "107308862771865 processed\n",
      "172864471892552 processed\n",
      "180663791083120 processed\n",
      "28271485059106 processed\n",
      "261683006747890 processed\n",
      "121426486551523 processed\n",
      "139028728648447 processed\n",
      "208987624720865 processed\n",
      "147790950389381 processed\n",
      "138495562862686 processed\n",
      "247928547083693 processed\n",
      "262588566762668 processed\n",
      "180853884696213 processed\n",
      "19859547683322 processed\n",
      "159023274509168 processed\n",
      "205298006856593 processed\n",
      "227964845509425 processed\n",
      "22967306557743 processed\n",
      "31360332891350 processed\n",
      "109864465509954 processed\n",
      "280574352919673 processed\n",
      "160309738656907 processed\n",
      "38095778767180 processed\n",
      "121739171405202 processed\n",
      "34281750928589 processed\n",
      "213098962995487 processed\n",
      "142963017992698 processed\n",
      "173378795104478 processed\n",
      "163338874919824 processed\n",
      "281060477973977 processed\n",
      "179514201539624 processed\n",
      "184072207257139 processed\n",
      "39642212641503 processed\n",
      "262281579374724 processed\n",
      "200285183752515 processed\n",
      "14116118331301 processed\n",
      "213462121531335 processed\n",
      "40791208039588 processed\n",
      "182460347350268 processed\n",
      "193771328834319 processed\n",
      "256818848095787 processed\n",
      "109926125936756 processed\n",
      "264192541883035 processed\n",
      "129269737770686 processed\n",
      "166146137448717 processed\n",
      "133292692499348 processed\n",
      "18199244904065 processed\n",
      "243798725348912 processed\n",
      "122728160862451 processed\n",
      "276111009888318 processed\n",
      "218997104404214 processed\n",
      "107101171295763 processed\n",
      "29121207153566 processed\n",
      "133077229237040 processed\n",
      "205400306449682 processed\n",
      "191094570027473 processed\n",
      "201413364648259 processed\n",
      "27932326533321 processed\n",
      "43454304520674 processed\n",
      "254723834139809 processed\n",
      "225986789154693 processed\n",
      "40956208776469 processed\n",
      "219914076422138 processed\n",
      "48644462529257 processed\n",
      "150685606149730 processed\n",
      "112642119390057 processed\n",
      "146476575543971 processed\n",
      "116147736518263 processed\n",
      "251651298261529 processed\n",
      "213449209518698 processed\n",
      "278417339378522 processed\n",
      "145949333561164 processed\n",
      "146326297210523 processed\n",
      "42786489407134 processed\n",
      "149732026109185 processed\n",
      "106150487182509 processed\n",
      "264224707860061 processed\n",
      "141036313864550 processed\n",
      "30581813007233 processed\n",
      "109125569252691 processed\n",
      "123532078120874 processed\n"
     ]
    }
   ],
   "source": [
    "mmsi_list = list(trajectories['mmsi'].unique())\n",
    "\n",
    "for i in mmsi_list:\n",
    "    \n",
    "    uniform_traj = trip_segmentation_prep(trajectories,i)\n",
    "    \n",
    "    #apply distance function and create new column with distance calc\n",
    "    uniform_traj['distance'] = uniform_traj.apply(distance_calc,axis=1)\n",
    "    uniform_traj['km_per_hr'] = (uniform_traj['distance'] / 20) * 60\n",
    "    \n",
    "    uniform_traj.dropna(subset=['km_per_hr'],inplace=True) #drop NaN / duplicates\n",
    "    uniform_traj = uniform_traj[~uniform_traj['km_per_hr'].isin([np.nan, np.inf, -np.inf])]  #filter out infinity values\n",
    "    uniform_traj['stationary_flag'] = uniform_traj.apply(station_flag,axis=1) #\n",
    "\n",
    "    #create a cumulative sum that tags changes in stationary movement with a new id \n",
    "    uniform_traj['trip_id'] = (uniform_traj['stationary_flag'].shift(1) != \\\n",
    "                                uniform_traj['stationary_flag']).astype(int).cumsum() \n",
    "\n",
    "    #the trip id will reset for each boat, e.g. boat ABC will have trip_id = 2,4,6,..,n and boat ZYX will have trip_id = 2,4,6,etc.\n",
    "    uniform_traj['trip_id'] = uniform_traj.apply(trip_tag,axis=1) #reassign stationary values all as 0 in trip id\n",
    "    \n",
    "    #since you don't care when a boat is stationary we can drop these records\n",
    "    uniform_traj = uniform_traj[uniform_traj['stationary_flag']!=1]\n",
    "\n",
    "    #we also want to exclude short trips\n",
    "    trip_length = pd.DataFrame(uniform_traj.groupby('trip_id')['stationary_flag'].count()) #get trip length\n",
    "    trip_length.reset_index(inplace=True)\n",
    "\n",
    "    trip_segments = pd.merge(left=uniform_traj,right=trip_length,how='left',left_on='trip_id',right_on='trip_id')\n",
    "    trip_segments = trip_segments[trip_segments['stationary_flag_y']>12]\n",
    "    \n",
    "    trip_segments['lat_lon'] = trip_segments[['lat','lon']].values.tolist()\n",
    "    trip_segments = trip_segments[['lat_lon','trip_id']]\n",
    "    \n",
    "    trip_id_list = list(trip_segments['trip_id'].unique())\n",
    "\n",
    "    segmented_trips = []\n",
    "\n",
    "    for trip in trip_id_list:\n",
    "        trip_test = trip_segments[trip_segments['trip_id']==trip]\n",
    "        trips = list(trip_test['lat_lon'].tolist())\n",
    "        segmented_trips.append(trips)\n",
    "    \n",
    "    print(\"{} processed\".format(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export structured list\n",
    "#save the list object\n",
    "with open('segmented_trajectories', 'wb') as fp:\n",
    "    pickle.dump(segmented_trips, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearn",
   "language": "python",
   "name": "deeplearn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
